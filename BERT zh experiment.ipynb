{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# It isn't obvious from the start how to use BERT.\n",
    "\n",
    "Hmm here's some repo for Chinese NLP models https://github.com/lonePatient/awesome-pretrained-chinese-nlp-models\n",
    "\n",
    "Hmm let's just the pretrained one on huggingface https://huggingface.co/bert-base-chinese"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import BertForMaskedLM"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-chinese\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "model = AutoModelForMaskedLM.from_pretrained(PRETRAINED_MODEL_NAME)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# hmmm looking at some tutorial here\n",
    "\n",
    "Source: https://www.heywhale.com/mw/project/609f609f06b942001794ff44\n",
    "\n",
    "- [CLS]：在做分类任务时其最后一层的repr. 会被视为整个输入序列的repr.\n",
    "- [SEP]：有两个句子的文本会被串接成一个输入序列，并在两句之间插入这个token 以做区隔\n",
    "- [UNK]：没出现在BERT 字典里头的字会被这个token 取代\n",
    "- [PAD]：zero padding 遮罩，将长度不一的输入序列补齐方便做batch 运算\n",
    "- [MASK]：未知遮罩，仅在预训练阶段会用到"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "vocab = tokenizer.vocab\n",
    "print(\"vocab size: \", len(vocab))"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e04f4558f936>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vocab size: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "text = \"[CLS] 等到潮水 [MASK] 了，就知道谁沒穿裤子。\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(text)\n",
    "print(tokens[:10], '...')\n",
    "print(ids[:10], '...')\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[CLS] 等到潮水 [MASK] 了，就知道谁沒穿裤子。\n",
      "['[CLS]', '等', '到', '潮', '水', '[MASK]', '了', '，', '就', '知'] ...\n",
      "[101, 5023, 1168, 4060, 3717, 103, 749, 8024, 2218, 4761] ...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "ids"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[101,\n",
       " 5023,\n",
       " 1168,\n",
       " 4060,\n",
       " 3717,\n",
       " 103,\n",
       " 749,\n",
       " 8024,\n",
       " 2218,\n",
       " 4761,\n",
       " 6887,\n",
       " 6443,\n",
       " 3760,\n",
       " 4959,\n",
       " 6175,\n",
       " 2094,\n",
       " 511]"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "from transformers import BertForMaskedLM\n",
    "# 除了 tokens 以外我們還需要辨別句子的 segment ids\n",
    "tokens_tensor = torch.tensor([ids])  # (1, seq_len)\n",
    "segments_tensors = torch.zeros_like(tokens_tensor)  # (1, seq_len)\n",
    "maskedLM_model = BertForMaskedLM.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "\n",
    "# 使用 masked LM 估計 [MASK] 位置所代表的實際 token \n",
    "maskedLM_model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = maskedLM_model(tokens_tensor, segments_tensors)\n",
    "    predictions = outputs[0]\n",
    "    # (1, seq_len, num_hidden_units)\n",
    "del maskedLM_model\n",
    "\n",
    "# 將 [MASK] 位置的機率分佈取 top k 最有可能的 tokens 出來\n",
    "masked_index = 5\n",
    "k = 3\n",
    "probs, indices = torch.topk(torch.softmax(predictions[0, masked_index], -1), k)\n",
    "predicted_tokens = tokenizer.convert_ids_to_tokens(indices.tolist())\n",
    "\n",
    "# 顯示 top k 可能的字。一般我們就是取 top 1 当做预测值\n",
    "print(\"輸入 tokens ：\", tokens[:10], '...')\n",
    "print('-' * 50)\n",
    "for i, (t, p) in enumerate(zip(predicted_tokens, probs), 1):\n",
    "    tokens[masked_index] = t\n",
    "    print(\"Top {} ({:2}%)：{}\".format(i, int(p.item() * 100), tokens[:10]), '...')\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "輸入 tokens ： ['[CLS]', '等', '到', '潮', '水', '[MASK]', '了', '，', '就', '知'] ...\n",
      "--------------------------------------------------\n",
      "Top 1 (65%)：['[CLS]', '等', '到', '潮', '水', '来', '了', '，', '就', '知'] ...\n",
      "Top 2 ( 4%)：['[CLS]', '等', '到', '潮', '水', '过', '了', '，', '就', '知'] ...\n",
      "Top 3 ( 4%)：['[CLS]', '等', '到', '潮', '水', '干', '了', '，', '就', '知'] ...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from bertviz import model_view\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-chinese\", output_attentions=True)\n",
    "inputs = tokenizer.encode(\"[CLS] 等到潮水 [MASK] 了，就知道谁沒穿裤子。\", return_tensors='pt')\n",
    "outputs = model(inputs)\n",
    "attention = outputs[-1]\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[0]) \n",
    "model_view(attention, tokens)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bertviz'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tt/63g2nkx15fl855gr711x66540000gn/T/ipykernel_90812/1572180877.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbertviz\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_view\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-chinese\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForMaskedLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-chinese\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bertviz'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "model"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=21128, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "inputs = tokenizer(['这是什么让我试试切词。你好', '这是什么让我试试切词。你好了卡就是打开了放假啊可是你的肌肤'], return_tensors=\"pt\", padding=True)\n",
    "outputs = model(**inputs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "inputs"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 6821, 3221,  784,  720, 6375, 2769, 6407, 6407, 1147, 6404,  511,\n",
       "          872, 1962,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0],\n",
       "        [ 101, 6821, 3221,  784,  720, 6375, 2769, 6407, 6407, 1147, 6404,  511,\n",
       "          872, 1962,  749, 1305, 2218, 3221, 2802, 2458,  749, 3123,  969, 1557,\n",
       "         1377, 3221,  872, 4638, 5491, 5502,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "outputs"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=None, logits=tensor([[[ -7.9119,  -7.8119,  -7.8056,  ...,  -6.5529,  -6.1713,  -6.4862],\n",
       "         [ -8.2322,  -8.0884,  -8.0818,  ...,  -6.7162,  -6.2229,  -6.5252],\n",
       "         [-16.1794, -16.3523, -18.0674,  ...,  -8.4667,  -4.2078,  -5.2893],\n",
       "         ...,\n",
       "         [ -7.7972,  -7.5529,  -7.6924,  ...,  -4.1089,  -2.6128,  -3.1202],\n",
       "         [ -7.9204,  -7.5319,  -7.7652,  ...,  -4.1665,  -2.4351,  -3.1046],\n",
       "         [ -7.9449,  -7.6439,  -7.8875,  ...,  -4.2426,  -2.6454,  -3.0096]],\n",
       "\n",
       "        [[ -8.0285,  -7.9644,  -7.9966,  ...,  -6.9183,  -6.6847,  -7.0758],\n",
       "         [ -8.1193,  -7.9995,  -7.9447,  ...,  -6.7129,  -6.3368,  -6.5415],\n",
       "         [-16.6799, -16.4066, -18.4110,  ...,  -9.9241,  -4.0629,  -5.6410],\n",
       "         ...,\n",
       "         [-13.4532, -13.5211, -12.6752,  ...,  -9.8302,  -4.2376, -10.3354],\n",
       "         [-10.6056, -10.2510, -10.3465,  ...,  -8.8303,  -3.1233,  -7.3241],\n",
       "         [-10.3576, -10.5163, -10.6644,  ...,  -8.2772,  -5.6076,  -8.5861]]],\n",
       "       grad_fn=<AddBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "outputs.logits.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2, 31, 21128])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "model.get_output_embeddings()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=21128, bias=True)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Okay not let's figure out how to use the outputs\n",
    "\n",
    "Sources: \n",
    "\n",
    "1. done https://towardsdatascience.com/bert-for-measuring-text-similarity-eec91c6bf9e1\n",
    "2. https://towardsdatascience.com/bert-text-classification-in-a-different-language-6af54930f9cb\n",
    "3. https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n",
    "4. https://towardsdatascience.com/how-bert-determines-search-relevance-2a67a1575ac4\n",
    "5. https://bergum.medium.com/how-not-to-use-bert-for-search-ranking-4586716428d9\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Now try to do some document ranking.\n",
    "\n",
    "Emm but no idea how to do that... No worries let's read some articles\n",
    "\n",
    "1. https://medium.com/@papai143/information-retrieval-with-document-re-ranking-with-bert-and-bm25-7c29d738df73\n",
    "2. https://medium.com/nerd-for-tech/bert-qe-contextualized-query-expansion-for-document-re-ranking-4f0f421840b9"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Try sentence transformer?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from pprint import pprint"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "sentences = ['This framework generates embeddings for each input sentence',\n",
    "    'Sentences are passed as a list of string.',\n",
    "    'The quick brown fox jumps over the lazy dog.']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "sentence_transformer_model = SentenceTransformer('distiluse-base-multilingual-cased-v1')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|██████████| 1.58M/1.58M [00:00<00:00, 14.1MB/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "sentence_embeddings = sentence_transformer_model.encode(sentences)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "#Print the embeddings\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentence: This framework generates embeddings for each input sentence\n",
      "Embedding: [-3.21109705e-02 -2.91681308e-02 -4.93781529e-02  5.35542425e-03\n",
      "  5.79125322e-02 -7.43310079e-02 -5.69196492e-02 -2.62950640e-02\n",
      " -2.77417712e-05 -4.68358630e-03 -5.46516441e-02 -9.72127309e-05\n",
      " -2.39549577e-03 -4.00913805e-02 -3.10327299e-02 -3.37153822e-02\n",
      " -7.05639832e-03  3.61287929e-02  2.57309480e-03 -5.69516756e-02\n",
      "  5.62542081e-02  1.25679923e-02  1.77145321e-02 -3.04215793e-02\n",
      " -2.37881746e-02 -2.86213458e-02  1.87028330e-02  1.17944861e-02\n",
      "  1.01970527e-02  1.55350352e-02  5.70734702e-02  4.81183529e-02\n",
      " -3.68592367e-02  1.59383602e-02 -6.20532595e-02  1.36246728e-02\n",
      " -2.07147878e-02  2.28246786e-02  3.50557789e-02  6.38704654e-03\n",
      " -2.75127441e-02  7.06582665e-02  3.77225950e-02 -3.34584806e-03\n",
      "  5.38096391e-02  6.05076626e-02  6.33056387e-02  9.52539325e-04\n",
      " -1.29994014e-02 -2.43680999e-02 -6.48297071e-02  2.89445352e-02\n",
      " -1.87204557e-03  9.60670318e-03 -1.06688216e-02 -8.95597041e-03\n",
      " -5.76722212e-02 -2.24711131e-02 -4.16073613e-02  1.55420480e-02\n",
      " -5.06275380e-03 -3.85802500e-02  4.78200316e-02  2.62673330e-02\n",
      " -1.81555245e-02  1.21903986e-01  8.18001851e-02 -3.71552235e-03\n",
      "  4.71419208e-02 -4.24777642e-02 -1.26148239e-02  7.73361772e-02\n",
      " -7.71055184e-03  3.78936864e-02 -4.55655269e-02 -6.91442937e-02\n",
      "  3.16633433e-02 -1.84740569e-03  2.71571167e-02 -1.65924262e-02\n",
      "  4.08282578e-02 -2.16219258e-02  3.16915251e-02 -3.05811074e-02\n",
      "  8.04238170e-02  4.67759036e-02  1.40082194e-02 -2.82813925e-02\n",
      "  2.66411602e-02  5.17893210e-02 -2.31178403e-02 -7.50948908e-03\n",
      "  2.90168840e-02  7.26427138e-02  6.70285989e-03 -4.16605696e-02\n",
      "  1.08104693e-02 -3.21918819e-03  8.81490260e-02  5.44506684e-02\n",
      "  1.26074245e-02 -4.72601801e-02 -3.12046371e-02  1.35354074e-02\n",
      " -9.68368957e-04  3.38522084e-02  5.16329659e-03 -2.67073754e-02\n",
      " -1.77091286e-02  2.52928045e-02  2.54465006e-02  1.69071004e-01\n",
      " -2.43682545e-02 -1.86561346e-02  2.80558430e-02  1.27587402e-02\n",
      "  2.49840990e-02  2.71386784e-02  1.29863009e-01  2.40882244e-02\n",
      "  1.41077838e-03  7.80556947e-02 -1.78298149e-02 -4.44321893e-02\n",
      "  7.14634126e-03 -1.34512028e-02  4.63873334e-02  2.09424030e-02\n",
      "  4.47379239e-03  3.68418358e-03 -1.64376979e-03  9.35932696e-02\n",
      " -4.71444353e-02  8.87603313e-03  3.86326835e-02  8.32491089e-03\n",
      " -2.71079149e-02 -3.18733566e-02  5.39869070e-02  6.33140951e-02\n",
      "  1.79902241e-02  2.40480229e-02 -2.34630667e-02 -9.68195591e-03\n",
      " -6.94372728e-02  1.62278134e-02  6.29155636e-02 -1.40329655e-02\n",
      " -3.50755788e-02 -6.19398542e-02  4.78709266e-02 -8.19381177e-02\n",
      " -2.36813184e-02  1.83961494e-03  4.48762774e-02 -3.97909619e-02\n",
      "  6.40089959e-02  2.79874522e-02  5.10933716e-03  8.26842114e-02\n",
      "  5.13132988e-03  1.01056121e-01  5.56323789e-02  6.00868836e-02\n",
      " -1.94657478e-04  4.79439311e-02  1.49257109e-02 -1.08983397e-01\n",
      " -8.10393039e-03 -3.05307955e-02  1.29013201e-02 -3.11966985e-02\n",
      "  5.23002297e-02  2.49553192e-03  1.00538423e-02 -5.56682097e-03\n",
      " -4.59135398e-02 -1.59336254e-02 -5.81627153e-02 -2.30473056e-02\n",
      "  4.24023010e-02 -1.26189385e-02 -5.76019287e-02 -2.35178806e-02\n",
      " -1.99224707e-02 -2.71408111e-02  6.18934184e-02 -7.12590069e-02\n",
      " -2.83036865e-02  1.39803961e-02 -6.02059029e-02 -2.99629942e-02\n",
      "  1.17682852e-02  6.52797520e-02  5.51759191e-02 -5.25569357e-02\n",
      "  3.12318187e-02 -1.82122942e-02  2.40754783e-02 -2.20054016e-02\n",
      "  8.17851126e-02  5.06472737e-02 -1.33394087e-02 -4.08101529e-02\n",
      " -3.22118923e-02  4.80807871e-02 -1.12104369e-02 -7.71349519e-02\n",
      " -3.72166447e-02 -6.93793297e-02  1.16011584e-02 -3.22417095e-02\n",
      " -7.40826223e-03 -3.24586928e-02 -1.38045140e-02 -2.15876959e-02\n",
      " -2.06397544e-03 -3.38051394e-02 -3.22641581e-02 -2.83756200e-02\n",
      " -9.19191167e-03 -2.91462801e-02 -2.28198450e-02 -1.70596875e-03\n",
      "  7.47529231e-03 -6.51658699e-02 -2.55198181e-02  3.43391299e-02\n",
      " -5.45136184e-02  6.87649772e-02  1.59350019e-02  1.35073168e-02\n",
      "  1.36039034e-01  4.97331396e-02 -4.42369096e-02 -4.15665954e-02\n",
      " -5.58694312e-03  4.72762622e-02  2.74596065e-02  1.19951377e-02\n",
      "  1.23891132e-02 -2.23171189e-02  4.91825156e-02  8.50712135e-03\n",
      " -5.90303130e-02  3.36036347e-02 -1.11529812e-01 -5.13930134e-02\n",
      "  1.29342973e-02  1.01932762e-02 -2.25533824e-02 -2.77077202e-02\n",
      " -7.66953975e-02 -4.34258347e-03  2.82431040e-02 -1.25168180e-02\n",
      "  2.24356446e-02 -5.24648726e-02  1.89032163e-02 -4.78619300e-02\n",
      " -9.15969629e-03  2.86992006e-02 -3.22315237e-03 -1.35177197e-02\n",
      " -4.14031744e-02 -5.93684167e-02 -6.30569756e-02 -1.22430176e-01\n",
      " -4.12621796e-02 -7.33231846e-03 -4.92018797e-02 -2.54869908e-02\n",
      " -5.61771542e-02 -2.77935881e-02 -1.88046526e-02 -3.55346836e-02\n",
      " -3.44540849e-02  2.37148032e-02 -3.24996077e-02 -3.61954328e-03\n",
      " -8.26535895e-02  4.37637605e-02 -5.68585619e-02  6.00125380e-02\n",
      " -2.06741802e-02 -1.93324022e-03  1.76415723e-02  1.30841723e-02\n",
      " -5.17140562e-03 -2.03503091e-02 -1.35852033e-02 -3.15684490e-02\n",
      " -4.47519496e-02 -5.44252023e-02  3.28855366e-02  2.78755706e-02\n",
      "  3.84406000e-02 -5.76061867e-02  4.39272113e-02  1.45973293e-02\n",
      "  2.14162245e-02 -4.24573198e-02 -4.45937365e-02  3.15970257e-02\n",
      " -6.01268075e-02  4.81789000e-03 -6.79689348e-02 -5.67853376e-02\n",
      "  1.79044679e-02  3.62378694e-02  1.06706671e-01  6.80869911e-03\n",
      " -5.64547330e-02  2.07506884e-02  2.23357398e-02 -6.04161434e-03\n",
      "  1.57316048e-02 -9.10231564e-03  2.32506413e-02  3.91208045e-02\n",
      " -2.81647835e-02 -6.07156381e-03 -9.22388304e-03  2.11056066e-03\n",
      "  1.15282960e-01  6.41370863e-02 -3.07731181e-02  3.80737870e-03\n",
      " -4.28212173e-02  2.20385157e-02 -1.03757538e-01 -4.67477664e-02\n",
      "  5.42338181e-04 -1.10421874e-01 -2.23168880e-02  7.45731145e-02\n",
      " -8.59444141e-02 -5.66241890e-02  6.08200841e-02  3.75657436e-03\n",
      " -1.72537584e-02  1.32132554e-02  3.51772048e-02  3.44948377e-03\n",
      " -3.38165425e-02 -3.54187675e-02  6.32639974e-03  6.28420105e-03\n",
      "  5.14720641e-02 -3.49637195e-02 -5.71757741e-02  1.70236845e-02\n",
      "  9.00868773e-02  1.40099674e-02 -3.71252224e-02 -2.34912578e-02\n",
      "  3.56149562e-02  2.70555262e-02 -1.12563306e-02  9.92395822e-03\n",
      "  3.68524566e-02  1.32297156e-02 -2.54338365e-02  5.92984725e-03\n",
      "  5.99444360e-02  4.88026924e-02  2.68273260e-02 -4.03694771e-02\n",
      "  1.02053538e-01  4.38278317e-02  3.13727707e-02 -9.22844186e-02\n",
      " -6.46953806e-02  8.12891871e-03  2.29737200e-02  2.47776750e-02\n",
      "  6.51381761e-02 -2.34639533e-02  7.65282363e-02  7.66328275e-02\n",
      "  2.03492567e-02 -4.63031456e-02  1.16651068e-02 -6.26534224e-03\n",
      " -7.63667980e-03 -1.89135093e-02  7.70138204e-03  8.19864275e-04\n",
      " -2.03506416e-03  3.73719707e-02  1.51718995e-02  1.55557925e-02\n",
      " -1.62855219e-02 -1.13889035e-02 -5.00953570e-02 -4.78942879e-02\n",
      " -6.50206627e-03  6.96800351e-02  2.15187427e-02 -1.64966695e-02\n",
      "  3.96237858e-02  6.25810539e-03 -9.49262157e-02 -5.64105285e-04\n",
      "  3.35824341e-02  3.27703319e-02  7.15626031e-02 -6.25048904e-03\n",
      " -3.40659125e-03  4.05281186e-02 -7.95072615e-02 -2.67423317e-02\n",
      "  8.01001489e-02  1.05417604e-02 -1.52032049e-02  5.99923730e-02\n",
      "  2.31071170e-02  6.34373575e-02 -1.64670479e-02 -5.80468215e-03\n",
      " -2.54075020e-03  5.14676869e-02  1.79309025e-02 -3.08595058e-02\n",
      "  5.52706569e-02  2.65074964e-03  2.00881381e-02 -5.85596636e-02\n",
      "  4.51424494e-02  4.37535755e-02  5.15959524e-02  1.26046753e-02\n",
      " -2.67690308e-02 -2.26988681e-02 -1.09378509e-02 -6.29564282e-03\n",
      "  2.98782159e-02  1.03279902e-02  1.83899477e-02  2.20715180e-02\n",
      "  1.64663941e-02  1.14165045e-01 -3.67216654e-02  6.73433915e-02\n",
      "  4.21112850e-02  3.11674587e-02 -2.32648989e-03  3.60182077e-02\n",
      " -2.84355711e-02 -7.05888215e-03 -3.85646150e-02  3.98614351e-03\n",
      " -2.38241516e-02  3.59631665e-02  7.44522666e-04 -3.90421762e-03\n",
      "  4.32992540e-02 -5.92641383e-02 -1.38429813e-02  5.12321852e-02\n",
      "  1.32191135e-02 -3.72545123e-02 -6.59616012e-03 -3.12289260e-02\n",
      " -9.33783408e-03  4.60976139e-02 -4.28920761e-02  9.47288349e-02\n",
      " -5.36447167e-02 -2.89178044e-02  5.84783545e-03 -7.38843717e-03\n",
      " -7.67531395e-02  1.79040222e-03  2.60678511e-02  1.42443981e-02\n",
      "  6.65723626e-03 -2.26613954e-02 -2.22207550e-02 -4.45660129e-02\n",
      " -2.57574487e-04  1.98222715e-02 -1.90616790e-02  1.59731340e-02\n",
      "  7.03947851e-03 -1.57165788e-02  1.60868745e-02 -1.87558886e-02\n",
      " -1.31529532e-02  3.52835320e-02  1.88668631e-02 -5.47016189e-02\n",
      " -1.82796158e-02  2.57096179e-02 -4.00732420e-02 -5.74829392e-02\n",
      " -6.77471757e-02 -2.93122251e-02 -2.18488295e-02  3.06316428e-02\n",
      " -1.65496711e-02  9.87859257e-03  4.20861831e-03  6.03941875e-03\n",
      " -4.87965569e-02  3.75490747e-02  1.93073954e-02  2.36933562e-03\n",
      " -9.25503019e-03 -3.64053436e-02 -1.96705572e-04 -3.73038352e-02]\n",
      "\n",
      "Sentence: Sentences are passed as a list of string.\n",
      "Embedding: [-5.78426383e-02  3.42108347e-02 -5.99114820e-02  4.87378193e-03\n",
      " -1.52740896e-01  7.76267145e-03  2.82022040e-02 -5.38134687e-02\n",
      "  5.15037626e-02  2.02300977e-02 -5.45511842e-02 -1.54919829e-02\n",
      "  1.01962965e-02  1.64574049e-02  3.96283297e-03 -2.37933230e-02\n",
      "  2.21821871e-02  2.23773066e-02 -1.16453394e-02  1.98942162e-02\n",
      " -7.54307164e-03 -6.53077960e-02 -2.03797985e-02  9.40525066e-03\n",
      " -7.59823294e-03 -3.32888402e-02 -8.85957778e-02  3.03784013e-02\n",
      " -1.41921267e-02 -1.69324931e-02  1.89997945e-02  5.19172065e-02\n",
      "  1.34596648e-02 -2.18769778e-02 -7.88779091e-03  1.14441989e-02\n",
      "  2.29125675e-02  3.70062590e-02  2.47222278e-03 -1.17591508e-02\n",
      " -2.21057404e-02  7.37542510e-02 -1.63042359e-03 -7.50945788e-03\n",
      " -1.46859242e-02  4.09931690e-02 -4.60377112e-02 -5.80365583e-02\n",
      "  1.51917003e-02 -2.63037682e-02 -3.33229564e-02  1.22492220e-02\n",
      " -9.30374116e-02  5.74827492e-02 -9.41971093e-02 -7.71343010e-03\n",
      " -8.08780920e-03 -7.62840509e-02 -3.01547535e-02 -3.26445960e-02\n",
      "  3.89937535e-02 -1.00396968e-01 -4.10998752e-03 -1.18209124e-02\n",
      "  7.36537427e-02  4.48357128e-02 -1.44898584e-02  4.31724638e-02\n",
      " -1.27445031e-02 -4.14786525e-02  6.84831366e-02  3.45707610e-02\n",
      "  9.72799771e-03  2.69216076e-02  6.94533065e-02 -2.17461190e-03\n",
      " -4.38050972e-03 -7.06411153e-03 -4.85959537e-02 -2.01040711e-02\n",
      "  2.79767867e-02 -2.37582740e-03 -1.25936046e-02  1.36265263e-03\n",
      "  5.89249693e-02  4.96636257e-02  1.12333754e-02 -1.40023762e-02\n",
      "  7.60958120e-02  2.83090922e-04 -2.93970220e-02  4.11545560e-02\n",
      "  4.22749445e-02  5.68960682e-02  7.55561963e-02  5.56637719e-02\n",
      "  1.06702559e-02  5.02032638e-02 -1.65955126e-02 -5.28975464e-02\n",
      " -2.71844175e-02 -7.15817809e-02 -2.15726607e-02 -4.30683307e-02\n",
      " -5.19390358e-03  1.32659764e-03 -4.26733494e-03  1.03904746e-01\n",
      "  7.91203696e-03  6.27293065e-02  5.17994128e-02 -8.32256973e-02\n",
      "  3.33245099e-02  1.34527897e-02  5.23365382e-03 -7.45321345e-03\n",
      "  4.05731844e-05  3.61738466e-02 -7.41865113e-02  8.83625727e-03\n",
      " -1.79691706e-02  8.19821805e-02 -3.33650820e-02 -8.92229564e-03\n",
      "  2.36391672e-03 -2.21797433e-02 -1.99473985e-02 -3.58422063e-02\n",
      " -6.33447096e-02  2.33306233e-02  1.65770277e-02  5.94506562e-02\n",
      " -9.41751599e-02 -1.27868736e-02  4.18283828e-02  6.62832037e-02\n",
      " -9.50480103e-02 -6.12974055e-02 -9.31736547e-03  6.38833866e-02\n",
      "  3.43689583e-02 -5.18294573e-02  2.39431299e-02 -3.21453772e-02\n",
      " -3.15300003e-02 -1.83909405e-02  5.12632541e-02 -3.57400248e-04\n",
      " -3.16023789e-02  3.18315700e-02 -2.27120239e-02 -1.21521227e-01\n",
      "  1.50557617e-02  2.48517822e-02  5.12134433e-02  1.03741325e-02\n",
      "  3.52816842e-02 -4.79334453e-03  6.90891519e-02  1.72404982e-02\n",
      " -4.98630442e-02 -6.42007738e-02  4.50166464e-02  9.50045139e-02\n",
      "  4.28572707e-02 -5.36456108e-02  1.37465261e-02 -9.44460705e-02\n",
      " -2.71603614e-02 -2.34779250e-02 -3.75760831e-02  2.34771450e-03\n",
      " -5.99581040e-02 -6.74029142e-02  5.89529378e-03 -3.16901132e-02\n",
      " -5.81443533e-02 -4.15979028e-02  5.40034287e-02  3.00307740e-02\n",
      "  5.24067041e-03  1.07924007e-01 -2.23463867e-02  1.21441884e-02\n",
      "  1.41574144e-02  4.86476794e-02 -3.53615321e-02 -1.30406963e-02\n",
      "  3.38831730e-02  3.11570577e-02 -3.02129262e-03  3.15910275e-03\n",
      " -7.74250925e-03 -6.18843734e-02  7.16332998e-03 -5.31732030e-02\n",
      "  1.42868996e-01 -1.07603455e-02 -1.29411779e-02 -1.60385463e-02\n",
      " -5.07626794e-02  7.48256296e-02 -4.02415618e-02 -7.41574634e-03\n",
      "  1.57941617e-02  5.83161525e-02 -4.10280563e-02 -3.01292278e-02\n",
      "  1.14015006e-02 -5.89346886e-02  3.33797485e-02  4.87593142e-03\n",
      "  5.65617112e-03 -7.49283433e-02 -1.19460123e-02  2.08449885e-02\n",
      "  1.16379336e-02  1.34305470e-02  1.46495746e-02 -6.40312582e-02\n",
      " -7.12049231e-02 -3.51377279e-02 -4.68018651e-02 -1.85523611e-02\n",
      " -4.23930306e-03  9.95141175e-03  1.55748765e-03  1.80234034e-02\n",
      " -1.07065849e-02  8.47222283e-03  3.39180604e-02 -2.15385985e-02\n",
      " -1.31744668e-02 -4.49803807e-02  8.63421336e-03  1.80524848e-02\n",
      "  8.84043574e-02  5.59115550e-03 -3.31658171e-03  5.23350723e-02\n",
      "  7.38946125e-02  3.78248072e-03  2.80679073e-02  5.12011312e-02\n",
      " -6.43964484e-02 -7.44843856e-02 -4.18867879e-02  8.84810649e-03\n",
      " -3.90512198e-02  4.19417322e-02 -2.21651010e-02 -1.17728431e-02\n",
      " -2.87149232e-02  1.17375078e-02 -3.68759334e-02 -4.50567231e-02\n",
      "  9.92158055e-03 -1.30501725e-02  4.49889936e-02 -1.81734376e-02\n",
      " -6.67764097e-02 -1.15647716e-02  3.00538377e-03 -1.63759906e-02\n",
      " -3.89745571e-02 -5.09673096e-02 -4.13273126e-02  6.32787421e-02\n",
      "  1.45166609e-02  6.16561770e-02 -5.34297675e-02  2.97756810e-02\n",
      " -5.73836640e-02 -2.12989785e-02  2.92014927e-02 -6.25262037e-05\n",
      " -7.31433555e-03 -7.79687380e-03 -4.56633680e-02 -2.30564717e-02\n",
      " -1.31386928e-02  6.29663409e-04 -8.72674771e-03 -4.61500045e-03\n",
      " -6.22813441e-02 -4.23874222e-02  5.39840572e-03 -1.85383856e-02\n",
      " -9.11470875e-03 -8.27711448e-03 -3.93106863e-02 -2.15602703e-02\n",
      " -4.43502553e-02 -1.32215787e-02 -1.73142329e-02  1.28327645e-02\n",
      "  8.18233043e-02 -4.65708636e-02  2.21664887e-02  8.64930972e-02\n",
      "  2.46858634e-02  3.74718122e-02  2.24540047e-02  2.88480651e-02\n",
      " -1.49040937e-03  7.47189042e-04  1.35686966e-02 -3.54265608e-02\n",
      " -6.54261112e-02  5.50219491e-02  6.07960373e-02  4.91422489e-02\n",
      "  2.57074255e-02  4.81997095e-02  5.76546974e-02 -3.20288278e-02\n",
      "  3.73655632e-02 -4.34054770e-02  1.58087388e-02  3.42958383e-02\n",
      "  3.11738509e-03  1.10631613e-02  3.30646262e-02 -2.74231099e-02\n",
      "  7.87989423e-02  8.39338824e-02 -5.59140109e-02 -4.05229628e-02\n",
      "  3.34423990e-03  3.12847234e-02 -1.09450251e-01  1.39849307e-02\n",
      " -3.51390876e-02 -2.65808143e-02  6.33211136e-02 -4.78233807e-02\n",
      " -3.90707031e-02 -9.49553028e-03 -1.90809313e-02 -1.14123700e-02\n",
      " -4.16522752e-03 -7.96086330e-04  3.22544873e-02 -4.96697836e-02\n",
      " -1.38623118e-02  2.30395794e-02  7.72054307e-03  1.30541269e-02\n",
      "  2.72500068e-02 -1.02515584e-02 -8.75074789e-02  4.59864214e-02\n",
      "  3.21710370e-02 -4.15291153e-02 -6.42369092e-02 -4.34734263e-02\n",
      "  1.48642259e-02  1.24743795e-02 -6.09974228e-02 -1.99863855e-02\n",
      " -4.96660080e-03 -4.99737523e-02  7.75287747e-02 -6.44929847e-03\n",
      "  1.66191754e-03  6.02384657e-02 -2.20969897e-02 -7.15991557e-02\n",
      " -3.84055049e-04  2.98359282e-02 -2.20723506e-02  1.49117119e-03\n",
      "  9.91246402e-02  4.96468507e-02  5.68753891e-02 -9.44850147e-02\n",
      "  3.75692733e-02 -6.56620935e-02  6.27980605e-02  1.10803777e-02\n",
      "  7.14609474e-02  5.53202145e-02  2.69677900e-02 -4.28046919e-02\n",
      "  7.97759444e-02 -6.44499576e-03  3.26727815e-02  4.79755849e-02\n",
      " -4.42408770e-02  5.50216362e-02 -2.92559224e-03 -2.12126356e-02\n",
      " -1.01331733e-02 -4.53836694e-02 -2.12917980e-02  7.23731844e-03\n",
      " -4.66950275e-02 -1.20057734e-02 -1.69921778e-02 -5.37029235e-03\n",
      "  8.19193386e-03  4.45900634e-02 -6.30585030e-02  4.17145081e-02\n",
      "  1.17084170e-02 -1.60901323e-02 -2.25794371e-02 -5.22942375e-03\n",
      "  1.84534546e-02  1.84892584e-02  6.25400618e-03  3.22606489e-02\n",
      "  6.49899691e-02  6.93959743e-03  4.03857306e-02 -1.40232174e-02\n",
      "  1.37463082e-02  1.13337368e-01  4.36259210e-02 -4.88359891e-02\n",
      "  4.34394431e-04 -7.83962235e-02 -4.68824282e-02 -4.11374345e-02\n",
      "  2.63941810e-02  1.02806101e-02 -9.22731776e-03 -3.32097113e-02\n",
      "  6.87412918e-02  8.70349072e-03  4.52272147e-02 -5.70025593e-02\n",
      "  6.76661506e-02 -5.47938282e-03  1.26270689e-02 -2.77299769e-02\n",
      "  2.14492455e-02  2.16324953e-03  2.80241836e-02 -3.23149981e-03\n",
      "  5.48090413e-02  6.19445220e-02 -1.39070665e-02  4.19943295e-02\n",
      "  2.84274910e-02  2.20864601e-02  1.79239530e-02  9.30539668e-02\n",
      " -5.89809790e-02  2.01635603e-02 -1.98372621e-02 -8.11016187e-02\n",
      " -4.20940481e-02 -4.17910982e-03  5.01890369e-02  3.41585949e-02\n",
      "  5.91820776e-02 -2.34658807e-03 -4.87404466e-02  7.47714266e-02\n",
      " -1.52791115e-02  7.51643069e-03 -4.88052666e-02 -8.13383684e-02\n",
      "  5.05572036e-02 -3.80737260e-02 -2.27894895e-02 -1.89287923e-02\n",
      " -7.23392656e-03 -1.39290560e-03  2.31851768e-02  3.41882668e-02\n",
      " -1.98201416e-03 -3.45658176e-02  7.41234869e-02 -1.34181995e-02\n",
      " -7.21451417e-02  5.99987283e-02 -3.69029306e-02  6.40857453e-03\n",
      " -3.57813500e-02  4.53238450e-02 -8.36520747e-04 -3.47037539e-02\n",
      " -2.39885561e-02 -8.70767534e-02 -4.47714031e-02 -1.42914113e-02\n",
      "  2.75537483e-02  1.81810874e-02 -2.12427117e-02 -5.95791936e-02\n",
      " -2.31691282e-02 -4.19487022e-02 -2.52823308e-02  2.02538539e-02\n",
      "  1.59321353e-02 -4.47279476e-02  4.68446948e-02  1.67115254e-03\n",
      "  5.53605333e-02  1.17172359e-03 -5.03638834e-02 -2.89350767e-02\n",
      " -5.95152285e-03 -2.29170509e-02 -3.75459641e-02 -4.23350297e-02\n",
      " -3.92323993e-02  5.46307079e-02 -1.33719239e-02 -1.33229736e-02]\n",
      "\n",
      "Sentence: The quick brown fox jumps over the lazy dog.\n",
      "Embedding: [ 0.0021956   0.01009582  0.07542428  0.00302132 -0.06788052 -0.04270106\n",
      " -0.02188476 -0.04076482  0.05281464 -0.02101998  0.012148    0.06688869\n",
      "  0.0033374   0.0636042  -0.03718344 -0.02322124 -0.05938825 -0.01645471\n",
      " -0.12036807  0.00621927  0.0296848  -0.05800417  0.02068966 -0.00771482\n",
      "  0.05882013  0.03520427 -0.02325829 -0.02316005  0.00169262  0.00135361\n",
      "  0.04765923  0.00759128  0.04927221  0.02833439 -0.05799786  0.03397731\n",
      "  0.05385162 -0.07574331  0.01731323 -0.02282482  0.02351133 -0.03370813\n",
      "  0.05369532  0.00054742 -0.00065109  0.00365539 -0.00714651 -0.01018885\n",
      "  0.05093229 -0.0128769  -0.04412914  0.11779792  0.00151305 -0.01959494\n",
      " -0.00138805  0.03488987 -0.05016121 -0.01500484  0.03360248  0.00396488\n",
      "  0.05674665 -0.00107875  0.06132217 -0.18768023  0.02762757 -0.05761081\n",
      "  0.02869146  0.00715614  0.02790288  0.06592703  0.01277214 -0.02951626\n",
      "  0.01149926 -0.09567575 -0.03133156  0.00679876 -0.00949144  0.0473044\n",
      " -0.02118545 -0.02098824  0.07057299 -0.01572308  0.05956693 -0.05095351\n",
      "  0.00688032 -0.02732856 -0.00387786  0.04009789  0.03045228 -0.07005028\n",
      "  0.07685568  0.01728462  0.00820142 -0.0498795  -0.00554011 -0.04379834\n",
      " -0.0059161  -0.01250118  0.00145488 -0.06107289 -0.01035024 -0.00250448\n",
      "  0.00700006 -0.04333888  0.04763111  0.05026086  0.01176572 -0.05213847\n",
      "  0.03169756 -0.03277942  0.06499284 -0.01902041  0.01296694 -0.01217613\n",
      "  0.08158133  0.05701136  0.04976162  0.03562326 -0.03787638  0.09632319\n",
      " -0.03999933  0.0500256   0.01078647 -0.04215484  0.02631541 -0.03813579\n",
      " -0.00690695  0.05541787  0.02349029 -0.01725571  0.05156013 -0.01003746\n",
      " -0.08462697 -0.05790202  0.01254475  0.03092253 -0.02539876 -0.04920597\n",
      "  0.04233776  0.01559691  0.03978441 -0.01369112  0.05215759 -0.02025764\n",
      " -0.05696872  0.02841539  0.08858877  0.04576198 -0.04788417  0.00714454\n",
      "  0.01917952 -0.0431136   0.01002246 -0.09034058  0.14767434  0.05853856\n",
      "  0.05314593  0.03103019 -0.01487324  0.00862091 -0.02211575 -0.01993713\n",
      " -0.04300544 -0.00098845 -0.07622766 -0.04311049  0.01498489 -0.05841988\n",
      "  0.0262881   0.02343118 -0.00155203  0.03209244  0.02299955 -0.03261461\n",
      " -0.04220801  0.01109378  0.0018021   0.00209926 -0.00056578  0.02988035\n",
      " -0.06050666  0.00496623 -0.01017073  0.04959828  0.02756973 -0.0273646\n",
      " -0.0223485   0.03086969  0.06229541  0.03573208 -0.01859458 -0.07670341\n",
      " -0.08330483 -0.05634426 -0.08007707 -0.03054598 -0.04380925  0.01608228\n",
      "  0.03481235  0.00321838  0.02286562 -0.07473312  0.0189488  -0.04465669\n",
      "  0.03680647 -0.05090664  0.02137366  0.00694563 -0.03761027  0.01657152\n",
      "  0.03972532 -0.04635761  0.08794423  0.09168507 -0.01526997 -0.02866159\n",
      " -0.03921528 -0.02238995  0.01977023 -0.08043354 -0.03370236 -0.03272631\n",
      " -0.04563705  0.03116073 -0.05718273 -0.07566708 -0.0513609   0.08820635\n",
      " -0.035106    0.02455368 -0.00844352  0.04702782 -0.06394574 -0.00934392\n",
      " -0.05163615 -0.04276062  0.0104286  -0.01472451 -0.00353355  0.000726\n",
      " -0.00815674  0.01523108 -0.08515702  0.0582554   0.00580196  0.00518282\n",
      " -0.00079001  0.01439049  0.00552709  0.02493188  0.00152134  0.0125959\n",
      " -0.04721165 -0.10193777  0.07168959 -0.05188907  0.00229853  0.02641364\n",
      " -0.00522945  0.02144036 -0.02914528 -0.04244187  0.01213161 -0.00437663\n",
      " -0.06286808 -0.01072994  0.03420288 -0.07765447 -0.02384716 -0.04163653\n",
      "  0.01930579 -0.03088382 -0.02533189 -0.06091263  0.02818073 -0.03867755\n",
      "  0.03733864 -0.02811597  0.02430369 -0.01028576 -0.03197465  0.03764495\n",
      " -0.07709509  0.04903011  0.03587156 -0.01174533 -0.00121957  0.05643995\n",
      "  0.03658674  0.02696557  0.01005826  0.00401214 -0.02805574 -0.03417901\n",
      "  0.06327311  0.01925071  0.00707067 -0.05617423 -0.05260286  0.04865647\n",
      "  0.01824123 -0.01887495  0.01635378 -0.01456437 -0.06510296  0.01425019\n",
      "  0.03454718 -0.00411989  0.01380149  0.03555726  0.01078077  0.02363158\n",
      "  0.01421545 -0.01559191 -0.00582433  0.00211504  0.03196087  0.03333754\n",
      "  0.05159938 -0.03123273  0.01244588 -0.05240429 -0.00343508 -0.07833381\n",
      " -0.00464683  0.03942405 -0.08245555  0.06891452  0.02800134  0.00155758\n",
      " -0.01782294 -0.069635    0.02413462  0.05783744  0.03928489  0.0182258\n",
      "  0.04155635  0.0543956  -0.05585474 -0.02155713  0.04596917 -0.02582825\n",
      "  0.09091378  0.00977654 -0.01426325 -0.04020213  0.02643902  0.0149759\n",
      " -0.03856286  0.03952531 -0.00778574  0.01498414  0.06167224 -0.03819492\n",
      "  0.02026251 -0.04995259  0.05037582  0.04132515  0.06980041 -0.03907128\n",
      " -0.0147726   0.01090392  0.06570213 -0.01403008  0.00809292  0.08917638\n",
      " -0.0228475  -0.0052192  -0.00650519 -0.0359022   0.04124722  0.02759026\n",
      "  0.01402584  0.05418929  0.03870736  0.01137648  0.01925811 -0.07541253\n",
      "  0.04335096 -0.08959006 -0.04678366 -0.00365412  0.00196794 -0.00741091\n",
      "  0.03900637  0.02881275 -0.10438446 -0.00126041 -0.07279758  0.038643\n",
      "  0.01231183  0.03848478 -0.02305092  0.07304481 -0.03741227  0.05925242\n",
      "  0.02734925  0.06735899 -0.05621493  0.02573763 -0.03143503  0.00037542\n",
      "  0.0233714   0.0914357   0.02539725 -0.05809261 -0.02244939 -0.01647673\n",
      "  0.03685328 -0.03615703  0.03323881 -0.04830455  0.01409299 -0.01748991\n",
      "  0.00213969  0.00804375 -0.03721574 -0.02165879  0.00256009 -0.01460751\n",
      " -0.04753616 -0.0245876   0.00657718  0.04597587  0.0104318   0.04706236\n",
      " -0.05095553  0.00535326 -0.00213497  0.02766475 -0.02311431 -0.00137512\n",
      "  0.00858548 -0.06238653 -0.04800393  0.02089921  0.05291415  0.04898107\n",
      "  0.05536468  0.05033444 -0.06873497  0.03966842 -0.0494011  -0.01558201\n",
      " -0.02431932 -0.0510637   0.00899732 -0.00146698  0.01234841  0.04812052\n",
      "  0.0117688  -0.02751712  0.04916232  0.010006   -0.03096954  0.02118216\n",
      "  0.003517   -0.04916179 -0.02082665  0.03019831  0.02935258  0.06377494\n",
      "  0.05402566 -0.0365555  -0.00742823  0.02895095  0.00642798  0.02851314\n",
      "  0.03209421  0.01445525 -0.04955629 -0.03148287  0.00248711 -0.01446532\n",
      " -0.04678696 -0.00342885 -0.104541   -0.00026772  0.11332572 -0.0392778\n",
      " -0.00142934 -0.05007633 -0.0797791  -0.01039788  0.06533416 -0.02180076\n",
      " -0.00816809  0.00413825  0.02277479 -0.01387191  0.00530034  0.02678911\n",
      " -0.00713742  0.05087418  0.00693968  0.04833124  0.00565551 -0.0006297\n",
      "  0.00955809 -0.00321598 -0.01309725 -0.03148219  0.004553    0.02162872\n",
      " -0.03347469 -0.00253257  0.07571241 -0.01989129  0.02724012 -0.00087924\n",
      " -0.00992554  0.03273376]\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "4b554b9bc7a45af9cb7affa56da8832edb06301fe4a315ecc0fd56b47822af07"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}