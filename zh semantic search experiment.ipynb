{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# It isn't obvious from the start how to use BERT.\n",
    "\n",
    "Hmm here's some repo for Chinese NLP models https://github.com/lonePatient/awesome-pretrained-chinese-nlp-models\n",
    "\n",
    "Hmm let's just the pretrained one on huggingface https://huggingface.co/bert-base-chinese"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import BertForMaskedLM"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-chinese\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "model = AutoModelForMaskedLM.from_pretrained(PRETRAINED_MODEL_NAME)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# hmmm looking at some tutorial here\n",
    "\n",
    "Source: https://www.heywhale.com/mw/project/609f609f06b942001794ff44\n",
    "\n",
    "- [CLS]：在做分类任务时其最后一层的repr. 会被视为整个输入序列的repr.\n",
    "- [SEP]：有两个句子的文本会被串接成一个输入序列，并在两句之间插入这个token 以做区隔\n",
    "- [UNK]：没出现在BERT 字典里头的字会被这个token 取代\n",
    "- [PAD]：zero padding 遮罩，将长度不一的输入序列补齐方便做batch 运算\n",
    "- [MASK]：未知遮罩，仅在预训练阶段会用到"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "vocab = tokenizer.vocab\n",
    "print(\"vocab size: \", len(vocab))"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e04f4558f936>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vocab size: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "text = \"[CLS] 等到潮水 [MASK] 了，就知道谁沒穿裤子。\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(text)\n",
    "print(tokens[:10], '...')\n",
    "print(ids[:10], '...')\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[CLS] 等到潮水 [MASK] 了，就知道谁沒穿裤子。\n",
      "['[CLS]', '等', '到', '潮', '水', '[MASK]', '了', '，', '就', '知'] ...\n",
      "[101, 5023, 1168, 4060, 3717, 103, 749, 8024, 2218, 4761] ...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "ids"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[101,\n",
       " 5023,\n",
       " 1168,\n",
       " 4060,\n",
       " 3717,\n",
       " 103,\n",
       " 749,\n",
       " 8024,\n",
       " 2218,\n",
       " 4761,\n",
       " 6887,\n",
       " 6443,\n",
       " 3760,\n",
       " 4959,\n",
       " 6175,\n",
       " 2094,\n",
       " 511]"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "from transformers import BertForMaskedLM\n",
    "# 除了 tokens 以外我們還需要辨別句子的 segment ids\n",
    "tokens_tensor = torch.tensor([ids])  # (1, seq_len)\n",
    "segments_tensors = torch.zeros_like(tokens_tensor)  # (1, seq_len)\n",
    "maskedLM_model = BertForMaskedLM.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "\n",
    "# 使用 masked LM 估計 [MASK] 位置所代表的實際 token \n",
    "maskedLM_model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = maskedLM_model(tokens_tensor, segments_tensors)\n",
    "    predictions = outputs[0]\n",
    "    # (1, seq_len, num_hidden_units)\n",
    "del maskedLM_model\n",
    "\n",
    "# 將 [MASK] 位置的機率分佈取 top k 最有可能的 tokens 出來\n",
    "masked_index = 5\n",
    "k = 3\n",
    "probs, indices = torch.topk(torch.softmax(predictions[0, masked_index], -1), k)\n",
    "predicted_tokens = tokenizer.convert_ids_to_tokens(indices.tolist())\n",
    "\n",
    "# 顯示 top k 可能的字。一般我們就是取 top 1 当做预测值\n",
    "print(\"輸入 tokens ：\", tokens[:10], '...')\n",
    "print('-' * 50)\n",
    "for i, (t, p) in enumerate(zip(predicted_tokens, probs), 1):\n",
    "    tokens[masked_index] = t\n",
    "    print(\"Top {} ({:2}%)：{}\".format(i, int(p.item() * 100), tokens[:10]), '...')\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "輸入 tokens ： ['[CLS]', '等', '到', '潮', '水', '[MASK]', '了', '，', '就', '知'] ...\n",
      "--------------------------------------------------\n",
      "Top 1 (65%)：['[CLS]', '等', '到', '潮', '水', '来', '了', '，', '就', '知'] ...\n",
      "Top 2 ( 4%)：['[CLS]', '等', '到', '潮', '水', '过', '了', '，', '就', '知'] ...\n",
      "Top 3 ( 4%)：['[CLS]', '等', '到', '潮', '水', '干', '了', '，', '就', '知'] ...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from bertviz import model_view\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-chinese\", output_attentions=True)\n",
    "inputs = tokenizer.encode(\"[CLS] 等到潮水 [MASK] 了，就知道谁沒穿裤子。\", return_tensors='pt')\n",
    "outputs = model(inputs)\n",
    "attention = outputs[-1]\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[0]) \n",
    "model_view(attention, tokens)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bertviz'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tt/63g2nkx15fl855gr711x66540000gn/T/ipykernel_90812/1572180877.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbertviz\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_view\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-chinese\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForMaskedLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-chinese\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bertviz'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "model"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=21128, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "inputs = tokenizer(['这是什么让我试试切词。你好', '这是什么让我试试切词。你好了卡就是打开了放假啊可是你的肌肤'], return_tensors=\"pt\", padding=True)\n",
    "outputs = model(**inputs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "inputs"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 6821, 3221,  784,  720, 6375, 2769, 6407, 6407, 1147, 6404,  511,\n",
       "          872, 1962,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0],\n",
       "        [ 101, 6821, 3221,  784,  720, 6375, 2769, 6407, 6407, 1147, 6404,  511,\n",
       "          872, 1962,  749, 1305, 2218, 3221, 2802, 2458,  749, 3123,  969, 1557,\n",
       "         1377, 3221,  872, 4638, 5491, 5502,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "outputs"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=None, logits=tensor([[[ -7.9119,  -7.8119,  -7.8056,  ...,  -6.5529,  -6.1713,  -6.4862],\n",
       "         [ -8.2322,  -8.0884,  -8.0818,  ...,  -6.7162,  -6.2229,  -6.5252],\n",
       "         [-16.1794, -16.3523, -18.0674,  ...,  -8.4667,  -4.2078,  -5.2893],\n",
       "         ...,\n",
       "         [ -7.7972,  -7.5529,  -7.6924,  ...,  -4.1089,  -2.6128,  -3.1202],\n",
       "         [ -7.9204,  -7.5319,  -7.7652,  ...,  -4.1665,  -2.4351,  -3.1046],\n",
       "         [ -7.9449,  -7.6439,  -7.8875,  ...,  -4.2426,  -2.6454,  -3.0096]],\n",
       "\n",
       "        [[ -8.0285,  -7.9644,  -7.9966,  ...,  -6.9183,  -6.6847,  -7.0758],\n",
       "         [ -8.1193,  -7.9995,  -7.9447,  ...,  -6.7129,  -6.3368,  -6.5415],\n",
       "         [-16.6799, -16.4066, -18.4110,  ...,  -9.9241,  -4.0629,  -5.6410],\n",
       "         ...,\n",
       "         [-13.4532, -13.5211, -12.6752,  ...,  -9.8302,  -4.2376, -10.3354],\n",
       "         [-10.6056, -10.2510, -10.3465,  ...,  -8.8303,  -3.1233,  -7.3241],\n",
       "         [-10.3576, -10.5163, -10.6644,  ...,  -8.2772,  -5.6076,  -8.5861]]],\n",
       "       grad_fn=<AddBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "outputs.logits.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2, 31, 21128])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "model.get_output_embeddings()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=21128, bias=True)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Okay not let's figure out how to use the outputs\n",
    "\n",
    "Sources: \n",
    "\n",
    "1. done https://towardsdatascience.com/bert-for-measuring-text-similarity-eec91c6bf9e1\n",
    "2. https://towardsdatascience.com/bert-text-classification-in-a-different-language-6af54930f9cb\n",
    "3. https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n",
    "4. https://towardsdatascience.com/how-bert-determines-search-relevance-2a67a1575ac4\n",
    "5. https://bergum.medium.com/how-not-to-use-bert-for-search-ranking-4586716428d9\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Now try to do some document ranking.\n",
    "\n",
    "Emm but no idea how to do that... No worries let's read some articles\n",
    "\n",
    "1. https://medium.com/@papai143/information-retrieval-with-document-re-ranking-with-bert-and-bm25-7c29d738df73\n",
    "2. https://medium.com/nerd-for-tech/bert-qe-contextualized-query-expansion-for-document-re-ranking-4f0f421840b9"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Try sentence transformer?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from pprint import pprint"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "sentence_transformer_model = SentenceTransformer('distiluse-base-multilingual-cased-v1')\n",
    "sentence_transformer_model_v2 = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "# later use this bigger model for higher performance 'paraphrase-multilingual-mpnet-base-v2'"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|██████████| 690/690 [00:00<00:00, 134kB/s]\n",
      "Downloading: 100%|██████████| 3.77k/3.77k [00:00<00:00, 671kB/s]\n",
      "Downloading: 100%|██████████| 723/723 [00:00<00:00, 586kB/s]\n",
      "Downloading: 100%|██████████| 122/122 [00:00<00:00, 39.7kB/s]\n",
      "Downloading: 100%|██████████| 229/229 [00:00<00:00, 29.0kB/s]\n",
      "Downloading: 100%|██████████| 1.11G/1.11G [08:03<00:00, 2.30MB/s]\n",
      "Downloading: 100%|██████████| 53.0/53.0 [00:00<00:00, 14.6kB/s]\n",
      "Downloading: 100%|██████████| 5.07M/5.07M [00:02<00:00, 1.94MB/s]\n",
      "Downloading: 100%|██████████| 239/239 [00:00<00:00, 76.7kB/s]\n",
      "Downloading: 100%|██████████| 9.08M/9.08M [00:38<00:00, 238kB/s]\n",
      "Downloading: 100%|██████████| 402/402 [00:00<00:00, 110kB/s]\n",
      "Downloading: 100%|██████████| 190/190 [00:00<00:00, 44.5kB/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "sentences = ['This framework generates embeddings for each input sentence',\n",
    "    'Sentences are passed as a list of string.',\n",
    "    'The quick brown fox jumps over the lazy dog.',\n",
    "    'Le renard brun et rapide saute par-dessus le chien paresseux.',\n",
    "    '敏捷的棕色狐狸跳过懒惰的狗',\n",
    "    'London is the best place on earth.',\n",
    "    'I love London.']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "sentence_embeddings = sentence_transformer_model_v2.encode(sentences)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "sentence_embeddings"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-0.00925649, -0.25025994, -0.01117302, ...,  0.13195269,\n",
       "        -0.16931531, -0.17440246],\n",
       "       [ 0.15494236, -0.06809119, -0.01351547, ...,  0.01244846,\n",
       "        -0.11615185, -0.12480734],\n",
       "       [-0.09213712,  0.08014815, -0.00743247, ...,  0.15812053,\n",
       "         0.255048  ,  0.03574583],\n",
       "       ...,\n",
       "       [-0.02064275,  0.12411997, -0.01361182, ...,  0.15744969,\n",
       "         0.1656531 ,  0.00059244],\n",
       "       [-0.11416887, -0.03790707, -0.01347593, ..., -0.00388702,\n",
       "         0.07200608, -0.02660454],\n",
       "       [-0.23245244,  0.07675917, -0.01163195, ...,  0.07803821,\n",
       "         0.03771931, -0.18306209]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "sentence_embeddings[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-9.25648678e-03, -2.50259936e-01, -1.11730229e-02,  1.17274500e-01,\n",
       "        1.07081749e-01,  5.40037313e-03,  1.46492511e-01,  1.44696152e-02,\n",
       "        1.06310651e-01,  6.33549914e-02,  2.63547003e-02,  2.65325576e-01,\n",
       "       -7.31476173e-02,  1.89226300e-01,  1.05784861e-02, -2.06184015e-01,\n",
       "        1.31849304e-01, -4.92107384e-02, -4.50022817e-02, -2.53913971e-03,\n",
       "        7.82639310e-02, -2.97641549e-02,  4.54539713e-03, -1.12729985e-02,\n",
       "       -1.20644532e-01,  2.43622176e-02, -1.43141404e-01,  2.72789598e-02,\n",
       "       -1.48834633e-02,  3.08458321e-02, -2.59506553e-02,  1.07542817e-02,\n",
       "       -8.04948136e-02, -1.60233527e-01,  6.63666204e-02,  2.12231167e-02,\n",
       "       -1.53289288e-01,  3.46428272e-03, -1.33140400e-01, -6.58026412e-02,\n",
       "       -1.31101459e-01, -8.27819705e-02,  5.41608520e-02,  2.45997291e-02,\n",
       "       -1.14434473e-01,  4.85093007e-03,  1.80670828e-01,  1.63245164e-02,\n",
       "        2.29559660e-01,  3.58686928e-04, -1.00527301e-01,  2.20974442e-02,\n",
       "       -1.46725690e-02,  1.22193441e-01,  4.00477760e-02, -1.53699368e-01,\n",
       "       -6.36366662e-03, -2.39348840e-02, -1.81877855e-02,  1.22280724e-01,\n",
       "       -1.00342073e-01, -4.79179509e-02,  1.88289843e-02, -8.88236910e-02,\n",
       "        2.78985184e-02, -3.92919667e-02, -1.91394575e-02, -2.12688744e-03,\n",
       "       -1.56745210e-01,  1.04388550e-01,  3.00291836e-01,  1.34791816e-02,\n",
       "        2.11487673e-02, -1.40897706e-01, -3.96654233e-02, -3.79050933e-02,\n",
       "       -7.91209340e-02,  2.16867365e-02, -2.07490064e-02,  6.08664528e-02,\n",
       "        1.63451314e-01,  4.04117219e-02, -6.52306452e-02,  7.34252855e-02,\n",
       "       -5.88078015e-02, -9.05694813e-02,  3.14204511e-03, -5.91315255e-02,\n",
       "       -2.14349583e-01,  1.38401175e-02,  9.66055468e-02, -1.27101600e-01,\n",
       "        1.13352668e-02, -6.52714297e-02, -5.95245287e-02,  5.39575191e-03,\n",
       "       -1.00438520e-01,  3.77513245e-02,  5.73557690e-02,  8.67233723e-02,\n",
       "        1.47595525e-01,  8.92543644e-02, -1.75924692e-02,  1.11088239e-01,\n",
       "        7.75808915e-02, -1.29427373e-01,  4.71465103e-02, -9.90661830e-02,\n",
       "       -6.36031702e-02,  3.33525278e-02, -1.14059327e-02, -1.77743539e-01,\n",
       "       -2.01473847e-01,  1.71841115e-01, -1.03610016e-01, -8.47272277e-02,\n",
       "       -1.72747381e-03,  1.47714302e-01, -5.22568822e-02, -5.53176664e-02,\n",
       "       -1.54099271e-01, -7.41737336e-02, -2.08843201e-02,  4.60943617e-02,\n",
       "        5.70717975e-02, -8.53623152e-02,  2.30844319e-02,  9.26613808e-02,\n",
       "       -3.68926837e-03, -4.46284190e-02, -1.18173972e-01,  5.65496497e-02,\n",
       "        4.04692302e-03,  2.48125438e-02, -6.56699017e-02,  1.96149311e-04,\n",
       "        5.44704162e-02,  4.21254970e-02,  5.88089526e-02, -1.28698081e-01,\n",
       "        1.10300137e-02, -4.81273159e-02, -8.73786286e-02, -2.19228908e-01,\n",
       "        1.33083805e-01,  6.71454743e-02, -4.71268333e-02, -8.15221220e-02,\n",
       "        6.35928884e-02, -2.80344766e-02,  5.69130965e-02,  9.46162418e-02,\n",
       "       -2.04838499e-01,  1.45992814e-02, -7.39906207e-02, -5.07961540e-03,\n",
       "        2.02874735e-01,  1.58263341e-01,  5.14318720e-02,  8.03609639e-02,\n",
       "       -1.98184941e-02,  1.02638952e-01, -9.39756706e-02,  1.69232432e-02,\n",
       "       -5.87568097e-02,  1.09248556e-01, -1.46171853e-01, -1.07695982e-01,\n",
       "        1.66370496e-01,  1.36441728e-02, -2.49671325e-01, -3.75698172e-02,\n",
       "       -7.54184201e-02,  3.17588359e-01, -4.53031361e-02,  2.77552873e-01,\n",
       "        6.71914220e-02,  4.52978611e-02,  1.19959377e-01,  4.91306372e-02,\n",
       "        2.92871129e-02,  2.25200817e-01,  6.49252459e-02,  5.80635965e-02,\n",
       "        1.52757000e-02, -1.29965134e-04,  1.79639254e-02, -5.43900020e-02,\n",
       "        6.81510046e-02, -4.98562716e-02,  1.12618603e-01,  1.10185809e-01,\n",
       "       -4.55069132e-02, -5.46206571e-02, -6.11410327e-02,  1.40818311e-02,\n",
       "       -4.97600846e-02,  1.36050299e-01, -1.36885131e-02, -1.33419707e-01,\n",
       "       -2.56264908e-03, -6.66363090e-02,  3.48147377e-02,  5.66679724e-02,\n",
       "        2.66382005e-02, -1.40907168e-01, -1.49921641e-01, -7.38056824e-02,\n",
       "        1.51898652e-01,  1.41071528e-01,  9.46449563e-02, -3.35702226e-02,\n",
       "        2.68974621e-02,  1.44904375e-01,  4.56527546e-02,  9.54592749e-02,\n",
       "        5.84382117e-02, -7.99941942e-02, -1.97007418e-01, -4.74941768e-02,\n",
       "       -9.89520550e-02,  3.19981724e-01,  1.67144388e-02,  5.33866473e-02,\n",
       "        2.57927980e-02, -6.79568201e-02,  2.38516089e-02, -2.76207160e-02,\n",
       "       -3.43162827e-02, -9.73500684e-02, -1.01706527e-01,  1.18560858e-01,\n",
       "        1.31721660e-01, -3.18316231e-03, -5.27090468e-02,  9.02977213e-02,\n",
       "       -3.71940047e-01, -2.37857848e-01,  1.88990496e-02, -5.06095588e-02,\n",
       "        1.01221941e-01, -4.36319530e-01,  8.88557581e-04,  2.63915621e-02,\n",
       "        3.22225019e-02, -3.35684493e-02,  2.27567226e-01,  1.64673291e-02,\n",
       "        3.59531417e-02,  1.34861621e-03, -1.06904963e-02,  2.67807953e-02,\n",
       "       -1.62987784e-03, -1.01844892e-01, -3.47828455e-02, -3.34265195e-02,\n",
       "       -7.25955591e-02,  4.50157858e-02,  5.81547432e-02,  1.33394793e-01,\n",
       "        6.51943013e-02,  6.72685131e-02,  1.56899411e-02, -2.28824783e-02,\n",
       "        7.39301592e-02, -3.14545557e-02, -2.82233711e-02, -2.78553098e-01,\n",
       "        5.13025373e-03, -4.29166202e-03,  3.79241584e-03,  8.03954378e-02,\n",
       "        4.79088025e-03,  6.15445664e-03,  2.95380987e-02,  3.07683069e-02,\n",
       "       -6.99599758e-02,  7.16385469e-02, -2.45730076e-02, -1.44093195e-02,\n",
       "       -9.86653380e-03,  4.62937094e-02, -2.98459977e-02, -3.41421105e-02,\n",
       "       -1.31376892e-01,  7.64959380e-02, -1.89056501e-01, -2.81518787e-01,\n",
       "       -1.96573704e-01, -2.96213865e-01, -4.62080576e-02,  7.09542930e-02,\n",
       "        1.79759473e-01, -1.36747390e-01,  5.04863895e-02, -5.34145497e-02,\n",
       "        2.47480702e-02,  2.07988426e-01,  2.35717613e-02,  1.70132384e-01,\n",
       "        5.68398722e-02, -1.57317985e-02, -1.00274704e-01, -1.09873647e-02,\n",
       "       -6.77048787e-02,  1.32865578e-01,  4.49286215e-02, -1.39109194e-01,\n",
       "        5.11654429e-02,  1.76914692e-01,  1.62913427e-01,  1.64844200e-01,\n",
       "       -1.30975973e-02,  2.81949103e-01, -4.54840399e-02,  7.09842984e-03,\n",
       "        1.05900764e-01, -1.87747315e-01,  1.84754450e-02,  2.82246858e-01,\n",
       "       -3.95943522e-02, -1.71277732e-01,  3.81272398e-02, -6.83396216e-03,\n",
       "       -9.46960319e-03,  3.81295457e-02, -5.85452691e-02, -1.54844612e-01,\n",
       "       -6.67580217e-02,  1.43654481e-01,  2.65273657e-02,  7.55394697e-02,\n",
       "        7.98689723e-02, -4.14943919e-02,  4.65814136e-02,  1.52792484e-01,\n",
       "        9.90260094e-02,  4.96233953e-03,  2.02019699e-02,  1.00624695e-01,\n",
       "        5.71266748e-02, -1.17313862e-01, -8.78674909e-02,  5.10938428e-02,\n",
       "        7.49378279e-02,  6.72976151e-02,  7.60923475e-02,  1.33465216e-01,\n",
       "       -1.09815309e-02, -5.42149842e-02,  1.66694477e-01, -1.36708051e-01,\n",
       "        9.97617282e-03, -2.61497889e-02, -1.26081273e-01,  2.76245978e-02,\n",
       "        1.15744255e-01, -1.95474196e-02,  5.76145947e-04, -1.51164010e-01,\n",
       "        1.85053065e-01,  9.99162197e-02, -5.05151264e-02,  1.84779897e-01,\n",
       "       -1.15464859e-01,  1.12800665e-01,  6.31479546e-02,  1.27319261e-01,\n",
       "       -1.02785565e-01, -3.52158323e-02,  3.47467745e-03,  2.08191443e-02,\n",
       "        4.75593805e-02,  4.41761985e-02,  1.19975492e-01,  1.21641442e-01,\n",
       "        1.14879772e-01, -1.20540261e-02, -8.15950111e-02,  8.49904027e-03,\n",
       "        1.12516217e-01, -1.48822412e-01,  9.95129421e-02,  2.76153199e-02,\n",
       "       -9.51129347e-02, -1.42558470e-01,  3.26235928e-02, -7.54420087e-02,\n",
       "       -6.42825291e-02,  1.21964589e-01, -3.28784510e-02,  6.50447905e-02,\n",
       "       -4.34150500e-03, -2.54501658e-03,  1.99617781e-02,  1.80083066e-01,\n",
       "       -1.92021713e-01,  8.96326825e-02, -2.73655001e-02,  3.40862535e-02,\n",
       "        1.77613683e-02,  9.49315354e-02, -1.70783494e-02, -3.44261914e-01,\n",
       "        7.18975663e-02,  1.55843616e-01,  6.11334387e-03,  9.53213796e-02,\n",
       "        3.73285636e-02,  1.60689965e-01,  7.32413456e-02, -2.66006380e-01,\n",
       "        2.20861938e-02, -4.14763317e-02,  2.71455441e-02, -7.41914660e-02,\n",
       "       -9.19713378e-02, -1.49265036e-01, -4.82338332e-02,  1.85182944e-01,\n",
       "       -1.21489130e-01, -7.80437365e-02,  1.22594200e-01, -5.84867001e-02,\n",
       "        1.50211319e-01, -7.60064349e-02,  3.11578531e-02,  2.19661184e-02,\n",
       "       -8.94017797e-03,  8.04376602e-02,  1.70824397e-02, -3.54327373e-02,\n",
       "       -3.13641392e-02, -1.31821275e-01, -5.25478981e-02,  1.29703775e-01,\n",
       "        1.21512217e-02, -2.89680988e-01, -5.05970325e-03, -1.75591394e-01,\n",
       "        9.47434828e-02,  1.80006862e-01,  2.74715632e-01, -2.19558571e-02,\n",
       "        1.38698027e-01, -1.97130889e-02, -1.24113150e-01, -1.22648582e-01,\n",
       "       -1.22392893e-01,  7.81004876e-02, -1.63548603e-03,  1.02091782e-01,\n",
       "       -7.67699480e-02,  4.72516529e-02, -5.88623099e-02,  5.28853536e-02,\n",
       "       -5.42995334e-02, -8.33883435e-02, -1.15883663e-01,  1.15041561e-01,\n",
       "       -1.87879540e-02,  2.90097952e-01, -5.46087250e-02, -1.56710614e-02,\n",
       "        1.25057483e-02, -1.77443195e-02,  6.20294213e-02, -1.22113779e-01,\n",
       "        2.75453627e-02,  8.75386000e-02, -7.51451477e-02,  2.84324009e-02,\n",
       "        8.53550658e-02,  1.04547456e-01,  1.37380525e-01, -3.48365749e-03,\n",
       "       -2.88497470e-02, -7.68249631e-02, -2.44978280e-03, -4.75462936e-02,\n",
       "        4.85345311e-02, -2.03669056e-01, -8.35199207e-02, -1.50838822e-01,\n",
       "        4.50152270e-02,  5.41365482e-02, -2.65972354e-02, -2.96230447e-02,\n",
       "       -1.00405850e-01,  4.97872755e-02,  4.62280167e-03,  1.01436421e-01,\n",
       "        3.51168886e-02,  2.69457281e-01, -1.59177288e-01,  1.87344387e-01,\n",
       "        2.17292562e-01,  7.39009827e-02, -5.56160472e-02, -1.39068082e-01,\n",
       "        5.86496666e-04,  6.47450015e-02,  2.15005912e-02, -6.85483217e-02,\n",
       "        1.11150026e-01, -4.19226363e-02, -1.37026161e-01,  1.54572397e-01,\n",
       "        1.26122847e-01,  3.57353762e-02, -5.33044897e-02,  1.45098835e-01,\n",
       "       -1.00141361e-01,  1.80284366e-01, -2.84566712e-02,  1.37922645e-01,\n",
       "       -1.25755280e-01, -6.23939326e-03,  9.05982926e-02,  1.35160178e-01,\n",
       "       -4.75118235e-02,  6.07467331e-02,  8.85854885e-02,  1.59674429e-03,\n",
       "        2.42147949e-02,  4.29205224e-02,  1.39417663e-01, -5.72590567e-02,\n",
       "       -4.72591519e-02,  9.35588852e-02, -1.60052180e-01,  4.37038466e-02,\n",
       "        7.45191798e-03, -9.72179323e-02, -7.07641020e-02,  8.25398937e-02,\n",
       "        9.29969400e-02,  1.27557084e-01,  5.60398959e-02, -1.01485588e-01,\n",
       "       -2.98373925e-04, -2.24869046e-02,  1.66566335e-02,  7.68572763e-02,\n",
       "        4.22000103e-02,  1.99679267e-02, -5.69290249e-03, -5.87710924e-02,\n",
       "        7.17421919e-02, -1.90273635e-02,  2.31059995e-02,  1.96227003e-02,\n",
       "       -7.89014548e-02,  2.97270752e-02,  1.98517159e-01, -9.89655927e-02,\n",
       "       -1.89898342e-01, -9.53426957e-02, -5.35678044e-02,  1.04563363e-01,\n",
       "       -3.91035490e-02, -1.32623166e-01, -2.19908860e-02,  8.54196474e-02,\n",
       "        6.09388724e-02, -5.98070323e-02,  4.00044397e-02, -1.21938409e-02,\n",
       "        9.47694108e-02, -4.94490452e-02,  6.37620464e-02, -2.57074125e-02,\n",
       "        3.41215432e-02,  5.83739989e-02, -9.94345397e-02,  3.47933322e-02,\n",
       "       -1.64795034e-02, -1.39040679e-01,  1.55050615e-02,  2.89589372e-02,\n",
       "        3.03126872e-02,  2.81894542e-02,  4.05223370e-02,  4.77378033e-02,\n",
       "        2.32724939e-04, -1.60426013e-02, -8.62300843e-02,  7.75063187e-02,\n",
       "       -2.21221782e-02,  2.74018824e-01,  9.77881532e-03,  1.40893564e-01,\n",
       "       -1.21830985e-01, -9.82237086e-02, -4.13901880e-02, -1.17378738e-02,\n",
       "       -1.50573298e-01,  2.55257189e-02,  1.76729947e-01, -2.09929235e-02,\n",
       "        1.08880237e-01, -8.06908496e-03, -2.55006906e-02,  7.99399242e-02,\n",
       "        2.40573920e-02,  2.36932620e-01, -2.04493731e-01, -5.59946634e-02,\n",
       "       -4.48534153e-02,  1.20736837e-01, -5.65658212e-02,  4.07087281e-02,\n",
       "        3.23604397e-03, -6.23376779e-02,  4.41867784e-02,  3.15065053e-03,\n",
       "       -1.63140818e-02,  4.96240146e-02,  3.72012220e-02, -9.67968330e-02,\n",
       "        5.45841269e-02,  6.48542121e-02,  1.11448236e-01,  5.30515723e-02,\n",
       "       -1.83473960e-01, -6.52527213e-02,  9.42969248e-02,  4.09186371e-02,\n",
       "        2.77902465e-02,  5.80556169e-02,  1.28629431e-02,  5.00651598e-02,\n",
       "       -1.29855156e-01, -8.49671848e-03, -2.48240419e-02,  2.63298571e-01,\n",
       "       -8.84218421e-03,  2.16445886e-02,  8.08991641e-02, -9.83721837e-02,\n",
       "       -2.08307549e-01, -8.68578926e-02, -4.21388680e-03, -1.96642607e-01,\n",
       "       -5.93193173e-02, -1.17096804e-01, -1.28552124e-01, -1.69709355e-01,\n",
       "        3.86424153e-03,  1.43606618e-01,  7.34155551e-02,  2.17737237e-04,\n",
       "       -5.46761639e-02, -9.13209394e-02, -1.05302133e-01,  1.17390826e-01,\n",
       "        3.96651737e-02,  3.30900885e-02,  2.74765305e-02, -6.02597259e-02,\n",
       "        5.92581891e-02,  4.69626896e-02,  1.32236019e-01,  2.94220783e-02,\n",
       "       -2.24629387e-01, -1.39417216e-01, -5.50492629e-02, -2.62367968e-02,\n",
       "       -1.18850626e-01,  1.52724296e-01,  7.56326318e-02, -9.56166759e-02,\n",
       "       -4.86505069e-02, -9.44743119e-03, -1.88924391e-02,  3.75633314e-02,\n",
       "       -6.48007765e-02,  2.06294493e-03,  1.09310940e-01,  1.60960525e-01,\n",
       "       -1.10010274e-01, -1.53192563e-03, -1.32460549e-01,  1.29825726e-01,\n",
       "        1.82785749e-01, -9.65213031e-02,  3.30877714e-02,  1.03960112e-01,\n",
       "        7.09584504e-02,  1.13452367e-01,  1.05946772e-01,  2.86519285e-02,\n",
       "        1.71353653e-01,  2.15042327e-02, -1.45538780e-03,  1.34097366e-02,\n",
       "        6.52541360e-03, -1.23162948e-01, -9.78679806e-02,  8.27502273e-03,\n",
       "       -5.06667467e-03,  5.89611866e-02,  1.24753594e-01, -1.35128140e-01,\n",
       "        2.06840962e-01,  9.84548125e-03, -5.15009277e-02, -1.20660618e-01,\n",
       "       -6.73844889e-02,  1.79515317e-01, -4.02814560e-02,  1.49995089e-01,\n",
       "        6.60192147e-02, -1.33930504e-01,  7.22322240e-02,  4.29007933e-02,\n",
       "       -5.68381250e-02, -1.26729300e-02, -2.37176474e-02, -2.86042392e-01,\n",
       "       -1.28334863e-02, -5.89607172e-02,  5.95758669e-02,  6.19206019e-02,\n",
       "        5.33733927e-02,  1.04327954e-01,  7.12594483e-03, -1.68297980e-02,\n",
       "        1.33746430e-01,  1.07140087e-01,  5.68080172e-02, -5.66988289e-02,\n",
       "       -4.64061797e-02,  1.11080751e-01, -2.42772311e-01,  5.40430434e-02,\n",
       "       -1.94963187e-01, -1.12625873e-02,  4.60618734e-02,  7.32073262e-02,\n",
       "       -2.81690694e-02,  4.58855275e-03, -4.41560149e-02, -4.11818922e-02,\n",
       "       -3.38207707e-02,  4.02318016e-02, -7.79237002e-02,  3.67999612e-03,\n",
       "        4.35621478e-02, -1.11833803e-01, -8.39930177e-02, -8.27049389e-02,\n",
       "       -3.72173847e-03,  1.07580302e-02, -2.57969439e-01, -6.09356798e-02,\n",
       "        1.31520350e-02,  1.21204387e-02, -2.58234441e-02,  1.74157605e-01,\n",
       "        2.26921991e-01, -4.91305850e-02, -9.80207399e-02, -2.02225342e-01,\n",
       "        7.25801140e-02,  1.31952688e-01, -1.69315308e-01, -1.74402460e-01],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "from sentence_transformers import util"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "print('sentences: ', sentences[0], '\\n', sentences[4])\n",
    "print('dot similarity: ', util.dot_score(sentence_embeddings[0], sentence_embeddings[4]))\n",
    "print('cos similarity: ', util.pytorch_cos_sim(sentence_embeddings[0], sentence_embeddings[4]))\n",
    "\n",
    "print('sentences: ', sentences[2], '\\n', sentences[4])\n",
    "print('dot similarity: ', util.dot_score(sentence_embeddings[2], sentence_embeddings[4]))\n",
    "print('cos similarity: ', util.pytorch_cos_sim(sentence_embeddings[2], sentence_embeddings[4]))\n",
    "\n",
    "print('sentences: ', sentences[3], '\\n', sentences[4])\n",
    "print('dot similarity: ', util.dot_score(sentence_embeddings[3], sentence_embeddings[4]))\n",
    "print('cos similarity: ', util.pytorch_cos_sim(sentence_embeddings[3], sentence_embeddings[4]))\n",
    "\n",
    "print('sentences: ', sentences[5], '\\n', sentences[6])\n",
    "print('dot similarity: ', util.dot_score(sentence_embeddings[5], sentence_embeddings[6]))\n",
    "print('cos similarity: ', util.pytorch_cos_sim(sentence_embeddings[5], sentence_embeddings[6]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sentences:  This framework generates embeddings for each input sentence \n",
      " 敏捷的棕色狐狸跳过懒惰的狗\n",
      "dot similarity:  tensor([[0.5263]])\n",
      "cos similarity:  tensor([[0.0797]])\n",
      "sentences:  The quick brown fox jumps over the lazy dog. \n",
      " 敏捷的棕色狐狸跳过懒惰的狗\n",
      "dot similarity:  tensor([[6.1115]])\n",
      "cos similarity:  tensor([[0.9069]])\n",
      "sentences:  Le renard brun et rapide saute par-dessus le chien paresseux. \n",
      " 敏捷的棕色狐狸跳过懒惰的狗\n",
      "dot similarity:  tensor([[5.9588]])\n",
      "cos similarity:  tensor([[0.9261]])\n",
      "sentences:  London is the best place on earth. \n",
      " I love London.\n",
      "dot similarity:  tensor([[8.4054]])\n",
      "cos similarity:  tensor([[0.7646]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# okay now get some passages out of ES to test this"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "test_query = '文书写不完怎么办'\n",
    "INDEX = \"efaqa-70\" # index to search e.g. \"msmacro-full\"\n",
    "FIELDS = [\"passage\"] # fields to search e.g. [\"passage\", \"query\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "from es_helper import *"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "result_count, es_hits = es_search(test_query, cutoff = 10, index=INDEX, fields = FIELDS)\n",
    "es_results = direct_es_search_result('dummy', test_query, es_hits)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/leonlu-m1/opt/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:209: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.13/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "es_results.table[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'rank': 1,\n",
       " 'qid': 'q0',\n",
       " 'pid': 'p0',\n",
       " 'query_label': '孩子上一年级了，因为考试成绩不好，没写完作业，不敢上学，心理承受能力差，怎么办我错了，现在考虑的是怎样让他开开心心的上学去我今天问了一下他幼儿园老师，了解到孩子从上小班对老师就很抵触，从不主动跟老师说话这次开学没写完作业，老师碰了他一下，就不去了在家玩了两天，做作业都成问题啦，老师都跟他说不打他，还在开学第一天的作业本上给他评了优呢，那也不去',\n",
       " 'passage': '孩子的问题肯定是家长的问题，你是怎么教育孩子的在面对学习这方面。你现在的想法太正确了，比起一些名牌大学的大学生还会跳楼自杀，孩子健康，尤其是心理健康，真的是最重要的，所以成绩不好要去鼓励他，作业写不完，可以帮着和老师解释一下',\n",
       " 'score': 12.461442}"
      ]
     },
     "metadata": {},
     "execution_count": 85
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "def rerank(es_results):\n",
    "    passages = [hit['passage'] for hit in es_results.table]\n",
    "    queries = [es_results.query_input]\n",
    "    query_embeddings = sentence_transformer_model_v2.encode(queries)\n",
    "    sentence_embeddings = sentence_transformer_model_v2.encode(passages)\n",
    "    reranked_ranking = util.semantic_search(query_embeddings, sentence_embeddings)\n",
    "\n",
    "    reranked_table = []\n",
    "    for entry_at_rank in reranked_ranking[0]:\n",
    "        print(entry_at_rank)\n",
    "        corpus_id = entry_at_rank['corpus_id']\n",
    "        entry = es_results.table[corpus_id]\n",
    "        entry['score'] = entry_at_rank['score']\n",
    "\n",
    "        reranked_table.append(entry)\n",
    "    return reranked_table\n",
    "        \n",
    "rerank(es_results)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'corpus_id': 8, 'score': 0.5146609544754028}, {'corpus_id': 7, 'score': 0.5065162777900696}, {'corpus_id': 1, 'score': 0.4716399312019348}, {'corpus_id': 0, 'score': 0.4393884837627411}, {'corpus_id': 5, 'score': 0.4194364547729492}, {'corpus_id': 9, 'score': 0.40949520468711853}, {'corpus_id': 6, 'score': 0.3897644281387329}, {'corpus_id': 4, 'score': 0.3841762840747833}, {'corpus_id': 3, 'score': 0.35703277587890625}, {'corpus_id': 2, 'score': 0.29797255992889404}]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-67b04fa6934b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mreranked_table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mrerank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mes_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-89-67b04fa6934b>\u001b[0m in \u001b[0;36mrerank\u001b[0;34m(es_results)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mentry_at_rank\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreranked_ranking\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry_at_rank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mcorpus_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry_at_rank\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'corpus_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mes_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry_at_rank\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "4b554b9bc7a45af9cb7affa56da8832edb06301fe4a315ecc0fd56b47822af07"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}