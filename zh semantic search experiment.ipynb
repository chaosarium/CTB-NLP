{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# It isn't obvious from the start how to use BERT.\n",
    "\n",
    "Hmm here's some repo for Chinese NLP models https://github.com/lonePatient/awesome-pretrained-chinese-nlp-models\n",
    "\n",
    "Hmm let's just the pretrained one on huggingface https://huggingface.co/bert-base-chinese"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# hmmm looking at some tutorial here\n",
    "\n",
    "Source: https://www.heywhale.com/mw/project/609f609f06b942001794ff44\n",
    "\n",
    "- [CLS]：在做分类任务时其最后一层的repr. 会被视为整个输入序列的repr.\n",
    "- [SEP]：有两个句子的文本会被串接成一个输入序列，并在两句之间插入这个token 以做区隔\n",
    "- [UNK]：没出现在BERT 字典里头的字会被这个token 取代\n",
    "- [PAD]：zero padding 遮罩，将长度不一的输入序列补齐方便做batch 运算\n",
    "- [MASK]：未知遮罩，仅在预训练阶段会用到"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-chinese\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "model = BertModel.from_pretrained(PRETRAINED_MODEL_NAME, output_attentions=True)\n",
    "inputs = tokenizer.encode(\"[CLS] 等到潮水涨起来了，就知道谁沒穿裤子。[SEP] 就知道谁沒穿裤子。 [SEP]\", return_tensors='pt')\n",
    "outputs = model(inputs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "outputs.last_hidden_state.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 768])"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "inputs = tokenizer(\"[CLS] 等到潮水涨起来了，就知道谁沒穿裤子。[SEP] 就知道谁沒穿裤子。 [SEP]\", return_tensors=\"pt\", padding=True)\n",
    "outputs = model(**inputs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "inputs"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101,  101, 5023, 1168, 4060, 3717, 3885, 6629, 3341,  749, 8024, 2218,\n",
       "         4761, 6887, 6443, 3760, 4959, 6175, 2094,  511,  102, 2218, 4761, 6887,\n",
       "         6443, 3760, 4959, 6175, 2094,  511,  102,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "outputs.last_hidden_state.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 768])"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Okay not let's figure out how to use the outputs\n",
    "\n",
    "Sources: \n",
    "\n",
    "1. done https://towardsdatascience.com/bert-for-measuring-text-similarity-eec91c6bf9e1\n",
    "2. https://towardsdatascience.com/bert-text-classification-in-a-different-language-6af54930f9cb\n",
    "3. https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n",
    "4. https://towardsdatascience.com/how-bert-determines-search-relevance-2a67a1575ac4\n",
    "5. https://bergum.medium.com/how-not-to-use-bert-for-search-ranking-4586716428d9\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Now try to do some document ranking.\n",
    "\n",
    "Emm but no idea how to do that... No worries let's read some articles\n",
    "\n",
    "1. https://medium.com/@papai143/information-retrieval-with-document-re-ranking-with-bert-and-bm25-7c29d738df73\n",
    "2. https://medium.com/nerd-for-tech/bert-qe-contextualized-query-expansion-for-document-re-ranking-4f0f421840b9"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "4b554b9bc7a45af9cb7affa56da8832edb06301fe4a315ecc0fd56b47822af07"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}